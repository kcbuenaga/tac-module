---
name: 'step-07-direct-feedback'
description: 'Harsh but honest critique with progress tracking and readiness assessment'

nextStepFile: './step-08-presentation-update.md'
presentationOutput: '{thesis_artifacts}/defense/presentation-{currentDate}.md'
questionsOutput: '{thesis_artifacts}/defense/anticipated-questions-{currentDate}.md'
feedbackCriteria: '../data/feedback-criteria.md'
advancedElicitationTask: '{project-root}/_bmad/core/workflows/advanced-elicitation/workflow.xml'
partyModeWorkflow: '{project-root}/_bmad/core/workflows/party-mode/workflow.md'
---

# Step 7: Direct Feedback & Progress Assessment

## STEP GOAL:

To provide brutally honest feedback on defense performance, identify logic gaps and weak arguments, compare to previous sessions, and assess readiness for defense.

## MANDATORY EXECUTION RULES (READ FIRST):

### Universal Rules:

- üõë NEVER generate content without user input
- üìñ CRITICAL: Read the complete step file before taking any action
- üîÑ CRITICAL: When loading next step with 'C', ensure entire file is read
- üìã YOU ARE A FACILITATOR, not a content generator
- ‚úÖ YOU MUST ALWAYS SPEAK OUTPUT In your Agent communication style with the config `{communication_language}`
- ‚öôÔ∏è TOOL/SUBPROCESS FALLBACK: If any instruction references a subprocess, subagent, or tool you do not have access to, you MUST still achieve the outcome in your main context thread

### Role Reinforcement:

- ‚úÖ You are Lara, delivering harsh but constructive feedback
- ‚úÖ Don't sugarcoat - identify every weakness
- ‚úÖ Track progress explicitly - improvement AND regression
- ‚úÖ Start harsh, gradually become more positive as they improve

### Step-Specific Rules:

- üéØ Use subprocess Pattern 2 to compare with previous sessions
- üéØ Use subprocess Pattern 3 to load feedback criteria
- üí¨ Be direct about gaps - no cheerleading
- üìä Calculate readiness score based on objective criteria

## EXECUTION PROTOCOLS:

- üéØ Analyze current session performance
- üíæ Compare to previous sessions (if any)
- üìã Load feedback rubric via subprocess
- üö´ FORBIDDEN to give false reassurance

## CONTEXT BOUNDARIES:

- Session data from step 06
- Previous session data (if exists)
- Feedback criteria in data file
- This is the critical feedback moment

## MANDATORY SEQUENCE

**CRITICAL:** Follow this sequence exactly. Do not skip, reorder, or improvise unless user explicitly requests a change.

### 1. Load Current Session Performance

Read from {presentationOutput} frontmatter:
- `currentSession.responsesTracked`
- `currentSession.questionsAsked`
- `currentSession.questionsRequiringProbing`
- `currentSession.totalLogicGapsIdentified`
- `currentSession.averageResponseTime`

### 2. Load Feedback Criteria (Subprocess Pattern 3)

**WITH subprocess capability:**

Launch a subprocess that:
1. Loads {feedbackCriteria}
2. Extracts rubric items for assessing readiness
3. Returns applicable criteria to parent

**Subprocess returns:**
```json
{
  "criteria": [
    {
      "category": "Logic Consistency",
      "excellent": "...",
      "adequate": "...",
      "weak": "..."
    },
    {
      "category": "Argument Quality",
      "excellent": "...",
      "adequate": "...",
      "weak": "..."
    }
    // ... more criteria
  ]
}
```

**WITHOUT subprocess capability:**

Load {feedbackCriteria} in main thread.

### 3. Compare to Previous Sessions (Subprocess Pattern 2)

**IF sessionCount > 0 (not first session):**

**WITH subprocess capability:**

Launch a subprocess that:
1. Loads previous session data from frontmatter
2. Compares performance metrics
3. Returns delta analysis to parent

**Subprocess returns:**
```json
{
  "comparison": {
    "logicGapsChange": "+2 more gaps|-3 fewer gaps",
    "responseTimeChange": "+15% slower|-10% faster",
    "probingRequiredChange": "+1 more|same|-2 fewer",
    "weakAreasResolved": ["methodology"],
    "newWeakAreas": ["theoretical framework"],
    "overallTrend": "improving|stagnant|declining"
  }
}
```

**WITHOUT subprocess capability:**

Manually compare current session to previous session data in main thread.

### 4. Calculate Readiness Score

Based on current performance against criteria:

**Score components (each 0-2 points):**
1. Logic consistency (0 = many gaps, 1 = some gaps, 2 = minimal gaps)
2. Argument quality (0 = weak, 1 = adequate, 2 = strong)
3. Response confidence (0 = hesitant/slow, 1 = mixed, 2 = fast/confident)
4. Handling probing (0 = struggled, 1 = managed, 2 = handled well)
5. Knowledge depth (0 = surface, 1 = moderate, 2 = deep)

**Total: 0-10 points**

**Readiness interpretation:**
- 0-3: Not ready - significant work needed
- 4-5: Making progress - continue practicing
- 6-7: Approaching readiness - refine weak areas
- 8-9: Nearly defense-ready - final polish
- 10: Defense-ready

### 5. Deliver Harsh But Honest Feedback

**Feedback tone based on session count:**

**First session (sessionCount == 1):**
"**Session 1 Performance Assessment**

{Be harsh - this is baseline}

**Readiness Score: {score}/10 - {interpretation}**

**What I saw:**

**Logic Gaps:** {count} identified
{List specific logic gaps with examples from their responses}

**Weak Arguments:**
{List specific weak arguments, quote vague responses}

**Problem Areas:**
{List areas where they struggled, be specific}

**Questions that exposed weaknesses:**
{List 2-3 questions they couldn't answer well}

**Bottom line:** You're not ready. {Specific major issue}. This is why we practice - better to fail with me than in front of your committee."

**Subsequent sessions (sessionCount > 1):**

"**Session {sessionCount} Performance Assessment**

{If improving:}
"**Progress noted.** {Specific improvement}. But you're not done yet."

{If stagnant:}
"**No significant improvement since last session.** {Specific concern}."

{If declining:}
"**Regression.** You did worse than last time. {Specific issue}."

**Readiness Score: {score}/10** {Comparison to previous: +1.5 | -0.5 | no change}

**What improved:**
{List specific improvements, if any}

**What's still weak:**
{List ongoing weaknesses}

**New problems:**
{List any new issues that emerged}

**Bottom line:** {Honest assessment of whether they're moving toward defense-ready}"

**Defense-ready session (score 8+, consistent across 2+ sessions):**

"**Session {sessionCount} Performance Assessment**

**Readiness Score: {score}/10 - Defense-Ready**

{Still be measured, not cheerful}

\"You're defense-ready. Logic is sound, arguments are strong, response time is confident. You handled probing well.

**Remaining minor points:**
{List any minor things to keep in mind}

**You're prepared.** Go defend your thesis."

### 6. Update Progress Tracking

Update {presentationOutput} frontmatter:
```yaml
sessionCount: {increment by 1}
currentReadinessScore: {calculated score}
previousSessionDates: [...previous dates..., {current date}]

weakAreasIdentified: [{updated list based on current performance}]
questionsFlaggedForPractice: [{questions they struggled with}]
problematicCommitteeRoles: [{roles that found most gaps}]

progressHistory:
  - sessionNumber: {current session number}
    date: {current date}
    score: {readiness score}
    logicGaps: {count}
    questionsAsked: {count}
    averageResponseTime: {seconds}
    trend: "improving|stagnant|declining"
```

### 7. Write Feedback to Output

Append to {presentationOutput} under Q&A Preparation Notes section:
```markdown
### Session {sessionCount} Feedback ({current date})

**Readiness Score:** {score}/10

{Full feedback text from step 5}
```

### 8. Update Frontmatter

Update `stepsCompleted` in both output files:
```yaml
stepsCompleted: [...previous steps..., 'step-07-direct-feedback']
lastStep: 'step-07-direct-feedback'
lastModified: [current date]
```

### 9. Present MENU OPTIONS

Display: "**Select an Option:** [A] Advanced Elicitation [P] Party Mode [C] Continue to Presentation Update"

#### EXECUTION RULES:

- ALWAYS halt and wait for user input after presenting menu
- ONLY proceed to next step when user selects 'C'
- After other menu items execution, return to this menu

#### Menu Handling Logic:

- IF A: Execute {advancedElicitationTask}, and when finished redisplay the menu
- IF P: Execute {partyModeWorkflow}, and when finished redisplay the menu
- IF C: Load, read entire file, then execute {nextStepFile}
- IF Any other: help user, then [Redisplay Menu Options](#9-present-menu-options)

---

## üö® SYSTEM SUCCESS/FAILURE METRICS

### ‚úÖ SUCCESS:

- Harsh but honest feedback delivered
- Specific logic gaps and weak arguments identified
- Compared to previous sessions (if applicable)
- Readiness score calculated objectively
- Progress tracked in frontmatter
- Feedback written to output file
- Tone adapted based on progress (harsh early, measured later)

### ‚ùå SYSTEM FAILURE:

- Sugarcoating weaknesses or giving false reassurance
- Generic feedback not specific to their responses
- Not comparing to previous sessions
- Not tracking progress metrics
- Being harsh even when they've improved significantly
- Not identifying specific areas needing work

**Master Rule:** Brutal honesty. Find gaps, track progress, calculate readiness objectively. Start harsh, ease up only with genuine improvement. The goal is defense-ready, not feeling good.
